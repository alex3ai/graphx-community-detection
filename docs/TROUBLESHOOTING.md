# üîß Guia de Troubleshooting - GraphX Community Detection Engine

## üìã √çndice R√°pido

1. [Problemas de Infraestrutura](#infraestrutura)
2. [Erros de Mem√≥ria](#mem√≥ria)
3. [Erros do Spark](#spark)
4. [Problemas com GraphFrames](#graphframes)
5. [Erros de Checkpoint](#checkpoint)
6. [Performance Issues](#performance)
7. [Valida√ß√£o de Dados](#dados)

---

## üê≥ Problemas de Infraestrutura

### ‚ùå Erro: "Container spark_master n√£o est√° rodando"

**Sintomas:**
```bash
Error: No such container: spark_master
```

**Diagn√≥stico:**
```bash
make status
docker ps -a | grep spark
```

**Solu√ß√µes:**

1. **Iniciar cluster:**
   ```bash
   make start
   ```

2. **Se container existir mas estar parado:**
   ```bash
   docker start spark_master spark_worker_1
   ```

3. **Se houver erro de porta em uso:**
   ```bash
   # Identificar processo usando porta 8080
   lsof -i :8080  # macOS/Linux
   netstat -ano | findstr :8080  # Windows
   
   # Matar processo ou mudar porta no docker-compose.yml
   ```

4. **Rebuild completo:**
   ```bash
   make stop
   docker-compose down -v
   make start
   ```

---

### ‚ùå Erro: "Health check failed"

**Sintomas:**
```
‚ö†Ô∏è  Master: N√£o respondendo
```

**Diagn√≥stico:**
```bash
docker logs spark_master
docker exec spark_master curl http://localhost:8080
```

**Causas Comuns:**
- Container iniciando (aguarde 30s)
- Mem√≥ria Docker insuficiente
- Conflito de portas

**Solu√ß√£o:**
```bash
# Verificar recursos Docker
docker info | grep -E "(Memory|CPUs)"

# Se mem√≥ria < 4GB, aumentar em Docker Settings
# macOS: Docker Desktop ‚Üí Preferences ‚Üí Resources
# Linux: /etc/docker/daemon.json
{
  "resources": {
    "memory": "6G"
  }
}

# Reiniciar Docker
sudo systemctl restart docker  # Linux
# Ou reiniciar Docker Desktop
```

---

## üíæ Erros de Mem√≥ria

### ‚ùå Erro: "Java Heap Space" ou "OutOfMemoryError"

**Sintomas:**
```
java.lang.OutOfMemoryError: Java heap space
Exception in thread "main" java.lang.OutOfMemoryError: GC overhead limit exceeded
```

**Diagn√≥stico:**
```bash
# Verificar recursos
make check-resources

# Verificar tamanho do dataset
make check-data
```

**Solu√ß√µes por Ordem de Prioridade:**

1. **Usar dataset menor:**
   ```bash
   make clean
   make generate-small  # 5k n√≥s ao inv√©s de 10k
   make process
   ```

2. **Aumentar mem√≥ria do Executor (docker-compose.yml):**
   ```yaml
   spark-worker:
     environment:
       - SPARK_WORKER_MEMORY=3G  # Era 2G
   ```

3. **Reduzir parti√ß√µes (community_detection.py):**
   ```python
   # Linha ~50
   shuffle_partitions = 50  # Era 100
   ```

4. **Desabilitar cache agressivo:**
   ```bash
   # Editar community_detection.py
   # Comentar linhas de cache:
   # g.vertices.cache()
   # g.edges.cache()
   ```

5. **Aumentar mem√≥ria Docker:**
   ```bash
   # Linux: editar /etc/docker/daemon.json
   {
     "default-runtime": "runc",
     "data-root": "/var/lib/docker",
     "storage-driver": "overlay2",
     "memory": "6442450944"  # 6GB em bytes
   }
   ```

---

### ‚ùå Erro: "Container exited with code 137" (OOM Killed)

**Causa:** Linux OOM Killer matou o processo.

**Solu√ß√£o:**
```bash
# 1. Verificar logs do sistema
dmesg | grep -i "killed process"

# 2. Aumentar swap (tempor√°rio)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 3. Usar dataset menor ou reduzir recursos
make clean
make generate-small
```

---

## ‚ö° Erros do Spark

### ‚ùå Erro: "graphframes package not found"

**Sintomas:**
```
java.lang.ClassNotFoundException: org.graphframes.GraphFrame
```

**Causa:** Maven package n√£o foi baixado.

**Solu√ß√µes:**

1. **Verificar conex√£o internet:**
   ```bash
   docker exec spark_master ping -c 3 repo1.maven.org
   ```

2. **Download manual:**
   ```bash
   docker exec spark_master spark-shell \
     --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12
   # Aguardar download, depois Ctrl+D
   ```

3. **Limpar cache Maven:**
   ```bash
   docker exec spark_master rm -rf /root/.ivy2/cache
   ```

4. **Especificar reposit√≥rio:**
   ```bash
   # Editar Makefile, adicionar:
   --repositories https://repo1.maven.org/maven2/
   ```

---

### ‚ùå Erro: Job "fica travado" em uma stage

**Sintomas:**
- UI mostra tasks pendentes por muito tempo
- Stage n√£o progride ap√≥s 5+ minutos

**Diagn√≥stico:**
```bash
# Acessar Spark UI
open http://localhost:4040

# Verificar:
# - Stages ‚Üí Tasks: quantas est√£o running vs pending?
# - Executors: todos workers conectados?
# - Storage: cache excessivo?
```

**Causas e Solu√ß√µes:**

1. **Worker n√£o conectado:**
   ```bash
   make status
   # Se worker n√£o aparecer:
   docker restart spark_worker_1
   ```

2. **Data skew (parti√ß√µes desbalanceadas):**
   ```bash
   # Editar community_detection.py, adicionar:
   .config("spark.sql.adaptive.skewJoin.enabled", "true")
   .config("spark.sql.adaptive.skewJoin.skewedPartitionFactor", "5")
   ```

3. **Muitas parti√ß√µes pequenas:**
   ```bash
   # Reduzir shuffle.partitions para 50
   # Ver se√ß√£o "Otimiza√ß√µes"
   ```

4. **Checkpoint muito grande:**
   ```bash
   make clean-checkpoints
   ```

---

### ‚ùå Erro: "Checkpoint directory not set"

**Sintomas:**
```
java.lang.IllegalStateException: Checkpoint directory has not been set
```

**Causa:** GraphFrames EXIGE checkpoint para algoritmos iterativos.

**Solu√ß√£o Definitiva:**
```bash
# 1. Verificar se volume est√° montado
docker exec spark_master ls -la /opt/spark-checkpoints

# 2. Se n√£o existir, criar manualmente:
docker exec spark_master mkdir -p /opt/spark-checkpoints
docker exec spark_master chmod 777 /opt/spark-checkpoints

# 3. Verificar c√≥digo (community_detection.py linha ~70):
spark.sparkContext.setCheckpointDir("/opt/spark-checkpoints")
```

**Refer√™ncia:** [GraphFrames User Guide - Checkpointing](https://graphframes.github.io/graphframes/docs/_site/user-guide.html#algorithms)

---

## üìä Problemas com GraphFrames

### ‚ùå Erro: "Label Propagation returned empty results"

**Causa:** Grafo desconectado ou configura√ß√£o incorreta.

**Diagn√≥stico:**
```python
# Verificar conectividade do grafo
results = g.connectedComponents()
num_components = results.select("component").distinct().count()
print(f"Componentes: {num_components}")

# Se > 1, grafo tem ilhas desconectadas
```

**Solu√ß√£o:**
```python
# Trabalhar apenas com maior componente
from pyspark.sql import Window

# Identificar maior componente
component_sizes = results.groupBy("component").count()
largest = component_sizes.orderBy(F.desc("count")).first()["component"]

# Filtrar grafo
vertices_filtered = results.filter(F.col("component") == largest)
edges_filtered = g.edges.join(
    vertices_filtered.select("id"),
    g.edges.src == vertices_filtered.id
)

g_filtered = GraphFrame(vertices_filtered, edges_filtered)
```

---

### ‚ùå Aviso: "LPA detectou comunidade dominante em scale-free"

**Sintoma:**
```
‚ö†Ô∏è  Comunidade dominante detectada (scale-free trait)
```

**Explica√ß√£o:** 
Label Propagation s√≠ncrono √© conhecido por convergir para comunidades gigantes em redes power-law devido √† influ√™ncia de hubs.

**Isso √© um bug?** 
N√£o! √â comportamento esperado documentado na literatura.

**Alternativas:**

1. **Usar algoritmo Louvain (mais robusto para scale-free):**
   ```bash
   # Atualmente n√£o implementado
   # Sugest√£o: contribuir com implementa√ß√£o!
   ```

2. **Usar Strongly Connected Components:**
   ```bash
   # J√° implementado no pipeline
   make process  # N√£o usar --skip-cc
   ```

3. **Ajustar maxIter do LPA:**
   ```bash
   # Menos itera√ß√µes podem evitar converg√™ncia prematura
   docker exec spark_master spark-submit ... \
     --lpa-iter 3  # Ao inv√©s de 5
   ```

**Refer√™ncia:** [Near linear time algorithm to detect community structures in large-scale networks](https://arxiv.org/abs/0709.2938)

---

## üîç Valida√ß√£o de Dados

### ‚ùå Erro: "Arestas com origem/destino inv√°lido"

**Sintomas:**
```
ValueError: ‚ùå Grafo inv√°lido: X arestas com src inv√°lido
```

**Diagn√≥stico:**
```bash
# Verificar integridade dos CSVs
head -20 data/input/vertices.csv
head -20 data/input/edges.csv

# Verificar IDs √∫nicos
cut -d',' -f1 data/input/vertices.csv | sort | uniq -d
```

**Causas:**
- Arquivo corrompido
- Encoding errado (UTF-8 esperado)
- V√≠rgulas dentro de valores

**Solu√ß√£o:**
```bash
# Regerar dados
make clean
make generate

# Se usando dados pr√≥prios:
# 1. Verificar formato CSV
# 2. Garantir colunas: id, src, dst
# 3. IDs devem ser strings √∫nicas
```

---

### ‚ùå Erro: "CSV parsing failed"

**Sintomas:**
```
pyspark.sql.utils.AnalysisException: CSV format error
```

**Solu√ß√£o:**
```bash
# 1. Verificar delimitador
file data/input/vertices.csv

# 2. Verificar encoding
file -i data/input/vertices.csv

# 3. Se necess√°rio, converter:
iconv -f ISO-8859-1 -t UTF-8 vertices.csv > vertices_utf8.csv

# 4. Remover caracteres especiais:
sed 's/[^a-zA-Z0-9,._-]//g' vertices.csv > vertices_clean.csv
```

---

## üöÄ Performance Issues

### üê¢ Job muito lento (>10min para 10k n√≥s)

**Benchmarks Esperados:**
- 5k n√≥s: ~2-3 minutos
- 10k n√≥s: ~5-7 minutos
- 50k n√≥s: ~15-25 minutos

**Diagn√≥stico:**
```bash
# 1. Acessar Spark UI
open http://localhost:4040/stages/

# 2. Identificar stage mais lenta
# 3. Verificar m√©tricas:
#    - Shuffle Read/Write
#    - GC Time
#    - Task distribution
```

**Otimiza√ß√µes:**

1. **Reduzir parti√ß√µes:**
   ```python
   # community_detection.py
   shuffle_partitions = 50  # Para 2 cores
   ```

2. **Habilitar AQE:**
   ```python
   .config("spark.sql.adaptive.enabled", "true")
   .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
   ```

3. **Ajustar cache:**
   ```python
   # Apenas para datasets pequenos
   g.vertices.cache().count()
   g.edges.cache().count()
   ```

4. **Desabilitar checkpoints desnecess√°rios:**
   ```python
   # Se LPA convergir r√°pido, reduzir checkpointInterval
   # Nota: N√ÉO remover checkpoint completamente!
   ```

5. **Usar SSD para checkpoint:**
   ```bash
   # Mudar volume no docker-compose.yml
   volumes:
     - /path/to/ssd/checkpoints:/opt/spark-checkpoints
   ```

---

### üìä Shuffle excessivo (>10GB para dataset pequeno)

**Diagn√≥stico:**
```bash
# Spark UI ‚Üí Stages ‚Üí Shuffle Read
# Se shuffle > 10x tamanho do dataset = problema
```

**Causas:**
- Muitas parti√ß√µes
- Joins sem broadcast
- Cache n√£o utilizado

**Solu√ß√µes:**
```python
# 1. Usar broadcast para joins pequenos
from pyspark.sql.functions import broadcast
df.join(broadcast(small_df), "key")

# 2. Repartir antes de joins
df = df.repartition(50, "join_key")

# 3. Persistir DataFrames reutilizados
df.persist(StorageLevel.MEMORY_AND_DISK)
```

---

## üÜò Comandos de Emerg√™ncia

### üî¥ Sistema completamente travado

```bash
# 1. For√ßar parada de tudo
docker kill $(docker ps -q)

# 2. Limpar recursos
make clean-all

# 3. Reiniciar Docker
sudo systemctl restart docker

# 4. Recome√ßar do zero
make setup-dev
make start
make generate-small
make process-fast
```

---

### üî¥ Erro desconhecido - coletar logs

```bash
# Coletar informa√ß√µes para debug
mkdir -p debug-logs

# Logs do Spark
docker logs spark_master > debug-logs/master.log 2>&1
docker logs spark_worker_1 > debug-logs/worker.log 2>&1

# Configura√ß√µes
docker exec spark_master env > debug-logs/env.txt
docker info > debug-logs/docker-info.txt
docker-compose config > debug-logs/compose-config.yml

# Compactar
tar -czf debug-$(date +%Y%m%d-%H%M%S).tar.gz debug-logs/

# Incluir em issue no GitHub
```

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial

1. **Apache Spark:**
   - [Performance Tuning](https://spark.apache.org/docs/3.5.0/sql-performance-tuning.html)
   - [Configuration](https://spark.apache.org/docs/3.5.0/configuration.html)
   - [Monitoring](https://spark.apache.org/docs/3.5.0/monitoring.html)

2. **GraphFrames:**
   - [User Guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)
   - [API Docs](https://graphframes.github.io/graphframes/docs/_site/api/python/index.html)

3. **Docker:**
   - [Resource Constraints](https://docs.docker.com/config/containers/resource_constraints/)

### Debugging Avan√ßado

```bash
# Modo debug verboso
docker exec spark_master spark-submit \
  --conf spark.log.level=DEBUG \
  --conf spark.eventLog.enabled=true \
  --conf spark.eventLog.dir=/opt/spark/logs \
  /opt/spark-apps/community_detection.py

# Analisar event logs
docker exec spark_master ls -lh /opt/spark/logs/
```

---

## ‚úÖ Checklist de Diagn√≥stico

Antes de reportar bug, verifique:

- [ ] `make validate` passa sem erros
- [ ] Docker tem ‚â•4GB RAM configurado
- [ ] Cluster est√° saud√°vel (`make health-check`)
- [ ] Dataset foi gerado corretamente (`make check-data`)
- [ ] Vers√µes corretas (Spark 3.5.0, GraphFrames 0.8.3)
- [ ] Logs coletados (`docker logs spark_master`)
- [ ] Tentou `make clean && make start`

---

**üí° Dica:** A maioria dos problemas se resolve com:
```bash
make clean-all && make setup-dev && make quick-test
```