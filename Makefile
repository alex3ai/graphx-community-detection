# GraphX Community Detection Engine - Makefile
# ‚úÖ MELHORIAS: Comandos otimizados, valida√ß√µes, documenta√ß√£o inline

.PHONY: help setup validate start stop status logs \
        generate generate-small generate-medium generate-large \
        process analyze benchmark clean clean-all \
        all test install-hooks health-check

# Vari√°veis
PYTHON := python3
PIP := pip3
DOCKER_COMPOSE := docker-compose
SPARK_MASTER := spark_master

# ‚úÖ Detectar n√∫mero de cores dispon√≠veis
CPU_CORES := $(shell nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 2)

# Cores para output
RED := \033[0;31m
GREEN := \033[0;32m
YELLOW := \033[1;33m
BLUE := \033[0;34m
CYAN := \033[0;36m
NC := \033[0m

##@ Ajuda

help: ## Exibe esta mensagem de ajuda
	@echo "$(BLUE)‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó$(NC)"
	@echo "$(GREEN)‚ïë  üï∏Ô∏è  GraphX Community Detection Engine - Comandos Dispon√≠veis        ‚ïë$(NC)"
	@echo "$(BLUE)‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù$(NC)"
	@awk 'BEGIN {FS = ":.*##"; printf "\n"} /^[a-zA-Z_-]+:.*?##/ { printf "  $(GREEN)%-20s$(NC) %s\n", $$1, $$2 } /^##@/ { printf "\n$(YELLOW)%s$(NC)\n", substr($$0, 5) } ' $(MAKEFILE_LIST)
	@echo ""
	@echo "$(CYAN)‚ÑπÔ∏è  Dica: Execute 'make quick-test' para valida√ß√£o r√°pida$(NC)"
	@echo ""

##@ Setup e Configura√ß√£o

validate: ## Valida ambiente de desenvolvimento
	@echo "$(BLUE)üîç Validando ambiente...$(NC)"
	@bash validate_environment.sh || (echo "$(RED)‚ùå Valida√ß√£o falhou!$(NC)" && exit 1)
	@echo "$(GREEN)‚úÖ Ambiente validado com sucesso!$(NC)"

setup: validate ## Instala depend√™ncias Python locais
	@echo "$(BLUE)üì¶ Instalando depend√™ncias...$(NC)"
	$(PIP) install -r requirements.txt --quiet
	@echo "$(GREEN)‚úÖ Depend√™ncias instaladas!$(NC)"

setup-dev: setup ## Setup completo para desenvolvimento
	@echo "$(BLUE)üîß Configurando ambiente de desenvolvimento...$(NC)"
	@mkdir -p data/{input,output,temp} analysis/{graphs,metrics} logs checkpoints
	@chmod -R 777 data analysis logs checkpoints 2>/dev/null || true
	@echo "$(GREEN)‚úÖ Estrutura de diret√≥rios criada!$(NC)"

install-hooks: ## Instala git hooks para valida√ß√£o
	@echo "$(BLUE)ü™ù Configurando git hooks...$(NC)"
	@mkdir -p .git/hooks
	@echo '#!/bin/bash\nmake validate' > .git/hooks/pre-commit
	@chmod +x .git/hooks/pre-commit
	@echo "$(GREEN)‚úÖ Git hooks instalados!$(NC)"

##@ Infraestrutura Docker

start: ## Inicia cluster Spark
	@echo "$(BLUE)üöÄ Iniciando cluster Spark...$(NC)"
	@$(DOCKER_COMPOSE) up -d
	@echo "$(YELLOW)‚è≥ Aguardando inicializa√ß√£o (20s)...$(NC)"
	@sleep 20
	@$(MAKE) health-check
	@echo ""
	@echo "$(GREEN)‚úÖ Cluster Spark iniciado!$(NC)"
	@echo "$(BLUE)‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó$(NC)"
	@echo "$(BLUE)‚ïë  üìä Spark Master UI: $(YELLOW)http://localhost:8080$(BLUE)                              ‚ïë$(NC)"
	@echo "$(BLUE)‚ïë  üìà Application UI:  $(YELLOW)http://localhost:4040$(BLUE) (quando job rodar)           ‚ïë$(NC)"
	@echo "$(BLUE)‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù$(NC)"

stop: ## Para cluster Spark
	@echo "$(YELLOW)üõë Parando cluster Spark...$(NC)"
	@$(DOCKER_COMPOSE) down
	@echo "$(GREEN)‚úÖ Cluster parado!$(NC)"

restart: stop start ## Reinicia cluster Spark

status: ## Mostra status dos containers
	@echo "$(BLUE)üìä Status dos containers:$(NC)"
	@$(DOCKER_COMPOSE) ps

health-check: ## ‚úÖ NOVO: Verifica sa√∫de do cluster
	@echo "$(BLUE)üè• Verificando sa√∫de do cluster...$(NC)"
	@if docker exec $(SPARK_MASTER) curl -sf http://localhost:8080 > /dev/null 2>&1; then \
		echo "$(GREEN)‚úÖ Master: Saud√°vel$(NC)"; \
	else \
		echo "$(RED)‚ùå Master: N√£o respondendo$(NC)"; \
		exit 1; \
	fi
	@if docker ps --filter "name=spark_worker" --format "{{.Names}}" | grep -q worker; then \
		echo "$(GREEN)‚úÖ Worker: Rodando$(NC)"; \
	else \
		echo "$(RED)‚ùå Worker: N√£o encontrado$(NC)"; \
		exit 1; \
	fi
	@echo "$(GREEN)‚úÖ Cluster saud√°vel!$(NC)"

logs: ## Exibe logs do Spark Master
	@$(DOCKER_COMPOSE) logs -f spark-master

logs-worker: ## Exibe logs do Spark Worker
	@$(DOCKER_COMPOSE) logs -f spark-worker

##@ Gera√ß√£o de Dados

generate-small: ## Gera dataset pequeno (5k n√≥s) - ~30s
	@echo "$(BLUE)üß¨ Gerando dataset PEQUENO (5,000 n√≥s)...$(NC)"
	@$(PYTHON) scripts/data_generator.py --nodes 5000 --avg-degree 4
	@echo "$(GREEN)‚úÖ Dataset gerado em data/input/$(NC)"

generate: generate-medium ## Alias para generate-medium

generate-medium: ## Gera dataset m√©dio (10k n√≥s) - ~1min
	@echo "$(BLUE)üß¨ Gerando dataset M√âDIO (10,000 n√≥s)...$(NC)"
	@$(PYTHON) scripts/data_generator.py --nodes 10000 --avg-degree 5
	@echo "$(GREEN)‚úÖ Dataset gerado em data/input/$(NC)"

generate-large: ## Gera dataset grande (50k n√≥s) - ~5min
	@echo "$(YELLOW)‚ö†Ô∏è  Gerando dataset GRANDE (50,000 n√≥s)...$(NC)"
	@$(PYTHON) scripts/data_generator.py --nodes 50000 --avg-degree 6
	@echo "$(GREEN)‚úÖ Dataset gerado em data/input/$(NC)"

generate-xlarge: ## ‚ö†Ô∏è  Dataset muito grande (100k n√≥s) - requer 6GB+ RAM
	@echo "$(RED)‚ö†Ô∏è  AVISO: Dataset MUITO GRANDE (100,000 n√≥s)$(NC)"
	@echo "$(YELLOW)   Requer pelo menos 6GB RAM dispon√≠vel$(NC)"
	@read -p "Continuar? (s/N): " confirm && [ "$$confirm" = "s" ] || exit 1
	@$(PYTHON) scripts/data_generator.py --nodes 100000 --avg-degree 8 --use-chunking
	@echo "$(GREEN)‚úÖ Dataset gerado em data/input/$(NC)"

generate-custom: ## Gera dataset customizado (use: make generate-custom NODES=20000 DEGREE=6)
	@echo "$(BLUE)üß¨ Gerando dataset CUSTOMIZADO...$(NC)"
	@$(PYTHON) scripts/data_generator.py --nodes $(NODES) --avg-degree $(DEGREE)

##@ Processamento

process: ## Executa pipeline Spark completo (~5min para dataset m√©dio)
	@echo "$(BLUE)‚öôÔ∏è  Executando pipeline Spark...$(NC)"
	@echo "$(YELLOW)‚è≥ Isso pode levar alguns minutos...$(NC)"
	@docker exec $(SPARK_MASTER) /opt/spark/bin/spark-submit \
		--master spark://spark-master:7077 \
		--executor-memory 2G \
		--driver-memory 1G \
		--packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
		--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
		--conf spark.sql.adaptive.enabled=true \
		/opt/spark-apps/community_detection.py
	@echo "$(GREEN)‚úÖ Pipeline conclu√≠do! Resultados em data/output/$(NC)"

process-fast: ## Executa pipeline r√°pido (menos itera√ß√µes) ~2min
	@echo "$(BLUE)‚ö° Executando pipeline R√ÅPIDO...$(NC)"
	@docker exec $(SPARK_MASTER) /opt/spark/bin/spark-submit \
		--master spark://spark-master:7077 \
		--executor-memory 2G \
		--driver-memory 1G \
		--packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
		/opt/spark-apps/community_detection.py \
		--pagerank-iter 5 \
		--lpa-iter 3 \
		--skip-cc
	@echo "$(GREEN)‚úÖ Pipeline r√°pido conclu√≠do!$(NC)"

process-optimized: ## ‚úÖ NOVO: Pipeline com auto-tuning ativo
	@echo "$(BLUE)üéØ Executando pipeline OTIMIZADO (auto-tuning)...$(NC)"
	@docker exec $(SPARK_MASTER) /opt/spark/bin/spark-submit \
		--master spark://spark-master:7077 \
		--executor-memory 2G \
		--driver-memory 1G \
		--packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
		--conf spark.sql.adaptive.enabled=true \
		--conf spark.sql.adaptive.skewJoin.enabled=true \
		/opt/spark-apps/community_detection.py
	@echo "$(GREEN)‚úÖ Pipeline otimizado conclu√≠do!$(NC)"

##@ An√°lise

analyze: ## Gera gr√°ficos e m√©tricas (~30s)
	@echo "$(BLUE)üìä Gerando an√°lises e visualiza√ß√µes...$(NC)"
	@$(PYTHON) scripts/analyze_communities.py
	@echo "$(GREEN)‚úÖ An√°lises conclu√≠das!$(NC)"
	@echo "$(BLUE)‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó$(NC)"
	@echo "$(BLUE)‚ïë  üìä Gr√°ficos: $(YELLOW)analysis/graphs/$(BLUE)                                        ‚ïë$(NC)"
	@echo "$(BLUE)‚ïë  üìà M√©tricas: $(YELLOW)analysis/metrics/$(BLUE)                                       ‚ïë$(NC)"
	@echo "$(BLUE)‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù$(NC)"

##@ Benchmarking

benchmark: ## Executa testes de performance (~15-20min)
	@echo "$(BLUE)üß™ Iniciando benchmark...$(NC)"
	@echo "$(YELLOW)‚ö†Ô∏è  Isso executar√° m√∫ltiplos jobs Spark (15-20 minutos)$(NC)"
	@$(PYTHON) scripts/benchmark_partitioning.py
	@echo "$(GREEN)‚úÖ Benchmark conclu√≠do! Resultados em analysis/metrics/$(NC)"

benchmark-quick: ## ‚úÖ NOVO: Benchmark r√°pido (3 configura√ß√µes) ~5min
	@echo "$(BLUE)üß™ Executando benchmark R√ÅPIDO...$(NC)"
	@echo "Testando apenas 3 configura√ß√µes cr√≠ticas..."
	@# Implementar vers√£o reduzida do benchmark
	@echo "$(YELLOW)‚ö†Ô∏è  Recurso em desenvolvimento$(NC)"

##@ Limpeza

clean: ## Remove dados gerados (mant√©m c√≥digo)
	@echo "$(YELLOW)üßπ Limpando dados gerados...$(NC)"
	@rm -rf data/input/*.csv
	@rm -rf data/output/*
	@rm -rf data/temp/*
	@rm -rf analysis/graphs/*.png
	@rm -rf logs/*.log
	@echo "$(GREEN)‚úÖ Dados limpos!$(NC)"

clean-checkpoints: ## ‚úÖ NOVO: Limpa apenas checkpoints
	@echo "$(YELLOW)üßπ Limpando checkpoints...$(NC)"
	@rm -rf checkpoints/*
	@docker exec $(SPARK_MASTER) rm -rf /opt/spark-checkpoints/* 2>/dev/null || true
	@echo "$(GREEN)‚úÖ Checkpoints limpos!$(NC)"

clean-all: clean clean-checkpoints ## Limpeza TOTAL (incluindo Docker cache)
	@echo "$(RED)üóëÔ∏è  Limpeza TOTAL...$(NC)"
	@$(DOCKER_COMPOSE) down -v
	@rm -rf analysis/metrics/*.csv analysis/metrics/*.json
	@docker system prune -f
	@echo "$(GREEN)‚úÖ Limpeza total conclu√≠da!$(NC)"

##@ Fluxos Completos

all: setup-dev start generate process analyze ## Pipeline COMPLETO (~10min)
	@echo ""
	@echo "$(GREEN)‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó$(NC)"
	@echo "$(GREEN)‚ïë  üéâ Pipeline completo executado com sucesso!                          ‚ïë$(NC)"
	@echo "$(GREEN)‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù$(NC)"
	@echo ""
	@echo "$(BLUE)üìä Pr√≥ximos passos:$(NC)"
	@echo "  ‚Ä¢ Visualize gr√°ficos em: $(YELLOW)analysis/graphs/$(NC)"
	@echo "  ‚Ä¢ Acesse Spark UI: $(YELLOW)http://localhost:8080$(NC)"
	@echo "  ‚Ä¢ Execute benchmark: $(YELLOW)make benchmark$(NC)"
	@echo ""

quick-test: setup-dev start generate-small process-fast analyze ## ‚úÖ Teste r√°pido (~3min)
	@echo "$(GREEN)‚úÖ Teste r√°pido conclu√≠do!$(NC)"

full-benchmark: setup-dev start generate benchmark ## Pipeline com benchmark (~20min)
	@echo "$(GREEN)‚úÖ Benchmark completo conclu√≠do!$(NC)"

##@ Utilit√°rios

shell-master: ## Abre shell no Spark Master
	@docker exec -it $(SPARK_MASTER) /bin/bash

shell-worker: ## Abre shell no Spark Worker
	@docker exec -it spark_worker_1 /bin/bash

check-data: ## ‚úÖ MELHORADO: Verifica dados com estat√≠sticas
	@echo "$(BLUE)üîç Verificando dados...$(NC)"
	@if [ -f "data/input/vertices.csv" ] && [ -f "data/input/edges.csv" ]; then \
		echo "$(GREEN)‚úÖ Dados de entrada encontrados$(NC)"; \
		echo "$(CYAN)Linhas:$(NC)"; \
		wc -l data/input/*.csv; \
		echo "$(CYAN)Tamanhos:$(NC)"; \
		du -sh data/input/*.csv; \
	else \
		echo "$(YELLOW)‚ö†Ô∏è  Dados n√£o encontrados. Execute: make generate$(NC)"; \
	fi
	@if [ -d "data/output/pagerank" ]; then \
		echo "$(GREEN)‚úÖ Resultados encontrados$(NC)"; \
		du -sh data/output/* 2>/dev/null; \
	else \
		echo "$(YELLOW)‚ö†Ô∏è  Resultados n√£o encontrados. Execute: make process$(NC)"; \
	fi

check-resources: ## ‚úÖ NOVO: Verifica recursos do sistema
	@echo "$(BLUE)üíª Recursos do Sistema:$(NC)"
	@echo "  ‚Ä¢ CPU Cores: $(CPU_CORES)"
	@echo "  ‚Ä¢ Mem√≥ria Total:" $$(free -h 2>/dev/null | awk '/^Mem:/{print $$2}' || echo "N/A")
	@echo "  ‚Ä¢ Mem√≥ria Dispon√≠vel:" $$(free -h 2>/dev/null | awk '/^Mem:/{print $$7}' || echo "N/A")
	@echo "  ‚Ä¢ Espa√ßo em Disco:" $$(df -h . | awk 'NR==2{print $$4}')
	@echo ""
	@echo "$(BLUE)üê≥ Recursos Docker:$(NC)"
	@docker info 2>/dev/null | grep -E "(Total Memory|CPUs)" || echo "$(YELLOW)Docker n√£o est√° rodando$(NC)"

version: ## Exibe vers√µes das ferramentas
	@echo "$(BLUE)üìã Vers√µes:$(NC)"
	@echo "  ‚Ä¢ Docker: $$(docker --version)"
	@echo "  ‚Ä¢ Docker Compose: $$(docker-compose --version)"
	@echo "  ‚Ä¢ Python: $$(python3 --version)"
	@echo "  ‚Ä¢ Pip: $$(pip3 --version)"
	@echo "  ‚Ä¢ CPU Cores: $(CPU_CORES)"

monitor: ## ‚úÖ NOVO: Monitora recursos em tempo real
	@echo "$(BLUE)üì° Monitorando recursos (Ctrl+C para sair)...$(NC)"
	@watch -n 2 'docker stats --no-stream'

##@ Desenvolvimento

test: validate check-data health-check ## Executa testes de valida√ß√£o
	@echo "$(GREEN)‚úÖ Todos os testes passaram!$(NC)"

watch-logs: ## Monitora logs em tempo real
	@$(DOCKER_COMPOSE) logs -f

debug: ## ‚úÖ NOVO: Modo debug com logs verbosos
	@echo "$(BLUE)üêõ Modo Debug$(NC)"
	@echo "Executando pipeline com logs verbosos..."
	@docker exec $(SPARK_MASTER) /opt/spark/bin/spark-submit \
		--master spark://spark-master:7077 \
		--packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
		--conf spark.log.level=INFO \
		/opt/spark-apps/community_detection.py

ci: validate test ## ‚úÖ NOVO: Simula√ß√£o de CI/CD
	@echo "$(GREEN)‚úÖ Checks de CI passaram!$(NC)"

.DEFAULT_GOAL := help