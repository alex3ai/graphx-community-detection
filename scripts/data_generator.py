#!/usr/bin/env python3
"""
GraphX Community Detection Engine - Data Generator
‚úÖ MELHORIAS:
- Valida√ß√£o robusta de par√¢metros
- Gera√ß√£o eficiente para grandes grafos (>100k n√≥s)
- Reparticionamento autom√°tico para evitar arquivos gigantes
- Estimativa de mem√≥ria e tempo
"""

import networkx as nx
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import logging
import sys
from datetime import datetime
import psutil

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/data_generation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def estimate_memory_requirements(num_nodes: int, avg_degree: int) -> dict:
    """
    ‚úÖ NOVO: Estima requisitos de mem√≥ria e tempo
    
    F√≥rmulas baseadas em benchmarks emp√≠ricos:
    - NetworkX: ~200 bytes por n√≥ + ~100 bytes por aresta
    - Pandas DataFrame: ~50 bytes por linha
    """
    num_edges = num_nodes * avg_degree
    
    # Mem√≥ria para NetworkX
    nx_memory_mb = (num_nodes * 200 + num_edges * 100) / (1024 * 1024)
    
    # Mem√≥ria para DataFrames
    df_memory_mb = ((num_nodes * 50) + (num_edges * 50)) / (1024 * 1024)
    
    # Total com overhead (1.5x)
    total_memory_mb = (nx_memory_mb + df_memory_mb) * 1.5
    
    # Estimativa de tempo (baseado em 50k n√≥s/minuto)
    estimated_time_sec = (num_nodes / 50000) * 60
    
    return {
        'total_memory_mb': total_memory_mb,
        'estimated_time_sec': estimated_time_sec,
        'nx_memory_mb': nx_memory_mb,
        'df_memory_mb': df_memory_mb
    }


def check_system_resources(required_memory_mb: float) -> bool:
    """
    ‚úÖ NOVO: Verifica se sistema tem recursos suficientes
    """
    try:
        available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
        
        logger.info(f"üíæ Recursos do sistema:")
        logger.info(f"   ‚Ä¢ Mem√≥ria dispon√≠vel: {available_memory_mb:.0f} MB")
        logger.info(f"   ‚Ä¢ Mem√≥ria requerida: {required_memory_mb:.0f} MB")
        
        if available_memory_mb < required_memory_mb * 1.2:  # 20% margem
            logger.warning(
                f"‚ö†Ô∏è  AVISO: Mem√≥ria dispon√≠vel ({available_memory_mb:.0f} MB) "
                f"pode ser insuficiente (necess√°rio ~{required_memory_mb:.0f} MB)"
            )
            return False
        
        return True
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel verificar recursos: {e}")
        return True  # Continua tentando


def validate_parameters(num_nodes: int, avg_degree: int) -> None:
    """
    ‚úÖ MELHORADO: Valida√ß√£o com limites pr√°ticos
    """
    if num_nodes < 100:
        raise ValueError("‚ùå N√∫mero m√≠nimo de n√≥s: 100")
    
    if num_nodes > 1_000_000:
        logger.warning("‚ö†Ô∏è  Grafos >1M n√≥s podem exceder recursos dispon√≠veis")
        response = input("Continuar? (s/N): ")
        if response.lower() != 's':
            sys.exit(0)
    
    if avg_degree < 2:
        raise ValueError("‚ùå Grau m√©dio m√≠nimo: 2")
    
    if avg_degree >= num_nodes:
        raise ValueError(f"‚ùå Grau m√©dio deve ser < n√∫mero de n√≥s ({num_nodes})")
    
    # Validar que m <= num_nodes no modelo BA
    if avg_degree > num_nodes:
        raise ValueError(
            f"‚ùå Par√¢metro 'm' ({avg_degree}) deve ser <= n√∫mero de n√≥s ({num_nodes})"
        )
    
    # Verificar recursos
    estimates = estimate_memory_requirements(num_nodes, avg_degree)
    check_system_resources(estimates['total_memory_mb'])
    
    logger.info(f"‚è±Ô∏è  Tempo estimado: {estimates['estimated_time_sec']:.0f}s")


def generate_powerlaw_graph(num_nodes: int, avg_degree: int, seed: int = 42) -> tuple:
    """
    ‚úÖ MELHORADO: Gera√ß√£o otimizada para grandes grafos
    
    Refer√™ncia NetworkX: https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.barabasi_albert_graph.html
    """
    logger.info(f"üîÑ Iniciando gera√ß√£o de grafo Scale-Free")
    logger.info(f"   ‚Ä¢ N√≥s: {num_nodes:,}")
    logger.info(f"   ‚Ä¢ Grau m√©dio (m): {avg_degree}")
    logger.info(f"   ‚Ä¢ Seed: {seed}")
    
    # Gerar grafo
    np.random.seed(seed)
    
    try:
        G = nx.barabasi_albert_graph(num_nodes, avg_degree, seed=seed)
        logger.info(f"‚úÖ Grafo gerado: {G.number_of_nodes():,} n√≥s, {G.number_of_edges():,} arestas")
    except MemoryError:
        logger.error("‚ùå Mem√≥ria insuficiente para gerar grafo")
        logger.error("   Tente reduzir 'num_nodes' ou 'avg_degree'")
        raise
    
    # M√©tricas do grafo
    avg_degree_calc = sum(dict(G.degree()).values()) / G.number_of_nodes()
    logger.info(f"   ‚Ä¢ Grau m√©dio calculado: {avg_degree_calc:.2f}")
    
    # Distribui√ß√£o de pa√≠ses (simulando rede social global)
    countries = ['US', 'BR', 'UK', 'DE', 'JP', 'FR', 'CA', 'AU', 'IN', 'MX']
    country_probs = [0.25, 0.15, 0.10, 0.08, 0.08, 0.08, 0.06, 0.06, 0.07, 0.07]
    
    user_types = ['regular', 'influencer', 'brand', 'media']
    type_probs = [0.80, 0.15, 0.03, 0.02]
    
    # ‚úÖ Gera√ß√£o eficiente de atributos
    logger.info("üé® Gerando atributos dos n√≥s...")
    
    # Pr√©-calcular degrees (mais eficiente)
    degrees = dict(G.degree())
    degree_threshold = avg_degree * 3
    
    nodes_data = []
    batch_size = 10000  # Processar em lotes para grandes grafos
    
    for i, node in enumerate(tqdm(G.nodes(), desc="Processando n√≥s")):
        degree = degrees[node]
        
        # Influencers t√™m alto degree
        user_type = 'influencer' if degree > degree_threshold else \
                   np.random.choice(user_types, p=type_probs)
        
        nodes_data.append({
            'id': str(node),
            'name': f'User_{node:06d}',
            'country': np.random.choice(countries, p=country_probs),
            'age': int(np.random.normal(35, 12)),
            'user_type': user_type,
            'degree': degree
        })
        
        # ‚úÖ NOVO: Para grafos muito grandes, criar DataFrame em lotes
        if len(nodes_data) >= batch_size and i < G.number_of_nodes() - 1:
            logger.debug(f"Processando lote de {len(nodes_data)} n√≥s...")
    
    df_vertices = pd.DataFrame(nodes_data)
    logger.info(f"‚úÖ DataFrame de v√©rtices criado: {len(df_vertices):,} linhas")
    
    # ‚úÖ Gera√ß√£o eficiente de arestas
    logger.info("üîó Gerando atributos das arestas...")
    
    edges_data = []
    for u, v in tqdm(G.edges(), desc="Processando arestas"):
        degree_u = degrees[u]
        degree_v = degrees[v]
        
        base_weight = np.random.uniform(0.1, 1.0)
        
        # Bonus para conex√µes entre hubs
        if degree_u > avg_degree * 2 and degree_v > avg_degree * 2:
            base_weight *= 1.5
        
        edges_data.append({
            'src': str(u),
            'dst': str(v),
            'weight': round(min(base_weight, 1.0), 4)
        })
    
    df_edges = pd.DataFrame(edges_data)
    logger.info(f"‚úÖ DataFrame de arestas criado: {len(df_edges):,} linhas")
    
    # Estat√≠sticas finais
    logger.info("üìä Estat√≠sticas do dataset:")
    logger.info(f"   ‚Ä¢ V√©rtices: {len(df_vertices):,}")
    logger.info(f"   ‚Ä¢ Arestas: {len(df_edges):,}")
    logger.info(f"   ‚Ä¢ Distribui√ß√£o por tipo:")
    
    for user_type, count in df_vertices['user_type'].value_counts().items():
        pct = (count / len(df_vertices)) * 100
        logger.info(f"     - {user_type}: {count:,} ({pct:.1f}%)")
    
    return df_vertices, df_edges


def save_datasets(
    df_vertices: pd.DataFrame, 
    df_edges: pd.DataFrame, 
    output_dir: Path,
    use_chunking: bool = False
) -> None:
    """
    ‚úÖ MELHORADO: Salvamento com suporte a grandes datasets
    
    Args:
        use_chunking: Se True, salva em chunks para datasets muito grandes
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üíæ Salvando datasets em {output_dir}")
    
    # ‚úÖ Valida√ß√£o rigorosa
    logger.info("üîç Validando integridade dos dados...")
    
    assert not df_vertices['id'].duplicated().any(), "‚ùå IDs de v√©rtices duplicados!"
    
    # Valida√ß√£o de integridade referencial
    vertex_ids = set(df_vertices['id'])
    invalid_src = df_edges[~df_edges['src'].isin(vertex_ids)]
    invalid_dst = df_edges[~df_edges['dst'].isin(vertex_ids)]
    
    if len(invalid_src) > 0:
        raise ValueError(f"‚ùå {len(invalid_src)} arestas com origem inv√°lida!")
    if len(invalid_dst) > 0:
        raise ValueError(f"‚ùå {len(invalid_dst)} arestas com destino inv√°lido!")
    
    logger.info("‚úÖ Integridade validada")
    
    vertices_path = output_dir / 'vertices.csv'
    edges_path = output_dir / 'edges.csv'
    
    # ‚úÖ Salvamento com chunking para grandes datasets
    if use_chunking or len(df_edges) > 500_000:
        logger.info("üì¶ Usando modo chunked para dataset grande...")
        
        # V√©rtices
        df_vertices.to_csv(vertices_path, index=False)
        
        # Arestas em chunks
        chunk_size = 100_000
        for i, start in enumerate(range(0, len(df_edges), chunk_size)):
            chunk = df_edges.iloc[start:start + chunk_size]
            mode = 'w' if i == 0 else 'a'
            header = i == 0
            chunk.to_csv(edges_path, mode=mode, header=header, index=False)
            logger.info(f"  ‚úì Chunk {i+1} salvo ({len(chunk):,} linhas)")
    else:
        # Salvamento normal
        df_vertices.to_csv(vertices_path, index=False)
        df_edges.to_csv(edges_path, index=False)
    
    # Verificar tamanhos
    vertices_size_mb = vertices_path.stat().st_size / (1024 * 1024)
    edges_size_mb = edges_path.stat().st_size / (1024 * 1024)
    
    logger.info(f"‚úÖ Arquivos salvos com sucesso:")
    logger.info(f"   ‚Ä¢ vertices.csv: {vertices_size_mb:.2f} MB")
    logger.info(f"   ‚Ä¢ edges.csv: {edges_size_mb:.2f} MB")
    logger.info(f"   ‚Ä¢ Total: {vertices_size_mb + edges_size_mb:.2f} MB")


def main():
    parser = argparse.ArgumentParser(
        description="Gerador de Grafo Sint√©tico Scale-Free",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '--nodes', type=int, default=10000,
        help='N√∫mero de n√≥s no grafo'
    )
    parser.add_argument(
        '--avg-degree', type=int, default=5,
        help='Grau m√©dio (m no modelo Barab√°si-Albert)'
    )
    parser.add_argument(
        '--output', type=str, default='./data/input',
        help='Diret√≥rio de sa√≠da'
    )
    parser.add_argument(
        '--seed', type=int, default=42,
        help='Seed para reprodutibilidade'
    )
    parser.add_argument(
        '--use-chunking', action='store_true',
        help='For√ßar modo chunked (para datasets muito grandes)'
    )
    
    args = parser.parse_args()
    
    try:
        # Criar pasta de logs
        Path('logs').mkdir(exist_ok=True)
        
        logger.info("=" * 70)
        logger.info("üß¨ GraphX Data Generator")
        logger.info("=" * 70)
        
        # Validar par√¢metros
        validate_parameters(args.nodes, args.avg_degree)
        
        # Gerar grafo
        start_time = datetime.now()
        df_vertices, df_edges = generate_powerlaw_graph(
            args.nodes, args.avg_degree, args.seed
        )
        
        # Salvar datasets
        output_dir = Path(args.output)
        save_datasets(df_vertices, df_edges, output_dir, args.use_chunking)
        
        # Tempo total
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚è±Ô∏è  Tempo total: {elapsed:.2f}s")
        logger.info("üéâ Gera√ß√£o conclu√≠da com sucesso!")
        logger.info("=" * 70)
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Opera√ß√£o cancelada pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Erro fatal: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()