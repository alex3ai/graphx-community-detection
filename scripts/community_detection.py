#!/usr/bin/env python3
"""
GraphX Community Detection Engine - Spark Processing Pipeline
‚úÖ OTIMIZA√á√ïES IMPLEMENTADAS:
- Checkpoint persistente
- Particionamento din√¢mico baseado em recursos
- Tratamento robusto de erros
- M√©tricas detalhadas de performance
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from graphframes import GraphFrame
import argparse
import time
import os
import sys
from datetime import datetime

os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable


def calculate_optimal_partitions(num_cores: int, dataset_size: str = 'medium') -> int:
    """
    ‚úÖ NOVO: Calcula n√∫mero √≥timo de parti√ß√µes baseado em recursos
    
    Refer√™ncia: Spark Performance Tuning Guide
    https://spark.apache.org/docs/3.5.0/sql-performance-tuning.html
    
    Regra: 2-5x n√∫mero de cores dispon√≠veis
    """
    size_multipliers = {
        'small': 2,   # <10k n√≥s
        'medium': 4,  # 10-50k n√≥s
        'large': 6    # >50k n√≥s
    }
    
    multiplier = size_multipliers.get(dataset_size, 4)
    optimal = num_cores * multiplier
    
    # Limites m√≠nimo e m√°ximo
    return max(8, min(optimal, 200))


def create_spark_session(
    app_name: str, 
    master: str, 
    shuffle_partitions: int = None,
    auto_tune: bool = True
) -> SparkSession:
    """
    ‚úÖ MELHORADO: Cria√ß√£o de sess√£o com auto-tuning
    """
    print("‚ö° Inicializando Spark Session com otimiza√ß√µes...")
    
    # Auto-detectar cores dispon√≠veis se n√£o especificado
    if shuffle_partitions is None and auto_tune:
        # Padr√£o conservador para recursos limitados
        shuffle_partitions = calculate_optimal_partitions(2, 'medium')
        print(f"  üéØ Auto-tuning: {shuffle_partitions} parti√ß√µes calculadas")
    elif shuffle_partitions is None:
        shuffle_partitions = 50  # Fallback conservador
    
    builder = SparkSession.builder \
        .appName(app_name) \
        .master(master) \
        .config("spark.jars.packages", "graphframes:graphframes:0.8.3-spark3.5-s_2.12")
    
    # Configura√ß√µes de serializa√ß√£o (Kryo obrigat√≥rio para GraphFrames)
    builder = builder \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryo.registrationRequired", "false") \
        .config("spark.kryoserializer.buffer.max", "256m")
    
    # ‚úÖ Particionamento otimizado
    builder = builder \
        .config("spark.sql.shuffle.partitions", str(shuffle_partitions)) \
        .config("spark.default.parallelism", str(shuffle_partitions))
    
    # ‚úÖ Adaptive Query Execution (cr√≠tico para grafos)
    # Refer√™ncia: https://spark.apache.org/docs/3.5.0/sql-performance-tuning.html#adaptive-query-execution
    builder = builder \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.adaptive.localShuffleReader.enabled", "true")
    
    # Configura√ß√µes de mem√≥ria
    builder = builder \
        .config("spark.memory.fraction", "0.6") \
        .config("spark.memory.storageFraction", "0.5")
    
    # ‚úÖ Cleanup autom√°tico de checkpoints
    builder = builder \
        .config("spark.cleaner.referenceTracking.cleanCheckpoints", "true")
    
    spark = builder.getOrCreate()
    
    # ‚úÖ CR√çTICO: Checkpoint persistente
    # Refer√™ncia GraphFrames: https://graphframes.github.io/graphframes/docs/_site/user-guide.html
    checkpoint_dir = "/opt/spark-checkpoints"
    
    # Criar diret√≥rio se n√£o existir (seguro no container)
    try:
        os.makedirs(checkpoint_dir, exist_ok=True)
        spark.sparkContext.setCheckpointDir(checkpoint_dir)
        print(f"  ‚úÖ Checkpoint configurado: {checkpoint_dir}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Aviso ao configurar checkpoint: {e}")
        # Fallback para /tmp se falhar
        spark.sparkContext.setCheckpointDir("/tmp/spark-checkpoints")
    
    # Log level
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"  ‚úÖ Spark {spark.version} inicializado")
    print(f"  ‚Ä¢ Master: {master}")
    print(f"  ‚Ä¢ Shuffle Partitions: {shuffle_partitions}")
    print(f"  ‚Ä¢ AQE: Enabled")
    
    return spark


def load_graph(spark: SparkSession, input_dir: str) -> GraphFrame:
    """
    ‚úÖ MELHORADO: Carregamento com valida√ß√£o robusta
    """
    print(f"\nüìÇ Carregando dados de: {input_dir}")
    
    vertices_path = f"{input_dir}/vertices.csv"
    edges_path = f"{input_dir}/edges.csv"
    
    try:
        # Carregar v√©rtices
        v = spark.read.csv(vertices_path, header=True, inferSchema=True)
        
        # Valida√ß√£o obrigat√≥ria
        if 'id' not in v.columns:
            raise ValueError("‚ùå V√©rtices devem conter coluna 'id'")
        
        vertex_count = v.count()
        print(f"  ‚úì V√©rtices: {vertex_count:,} registros")
        
        # Carregar arestas
        e = spark.read.csv(edges_path, header=True, inferSchema=True)
        
        if 'src' not in e.columns or 'dst' not in e.columns:
            raise ValueError("‚ùå Arestas devem conter 'src' e 'dst'")
        
        edge_count = e.count()
        print(f"  ‚úì Arestas: {edge_count:,} registros")
        
        # ‚úÖ Valida√ß√£o de integridade referencial
        print("  üîç Validando integridade do grafo...")
        vertex_ids = v.select("id").distinct()
        
        invalid_src = e.join(vertex_ids, e.src == vertex_ids.id, "left_anti")
        invalid_dst = e.join(vertex_ids, e.dst == vertex_ids.id, "left_anti")
        
        invalid_src_count = invalid_src.count()
        invalid_dst_count = invalid_dst.count()
        
        if invalid_src_count > 0 or invalid_dst_count > 0:
            raise ValueError(
                f"‚ùå Grafo inv√°lido: {invalid_src_count} arestas com src inv√°lido, "
                f"{invalid_dst_count} com dst inv√°lido"
            )
        
        print("  ‚úÖ Integridade validada")
        
        # Criar GraphFrame
        print("  üî® Construindo GraphFrame...")
        g = GraphFrame(v, e)
        
        # ‚úÖ Cache estrat√©gico
        # Refer√™ncia: https://spark.apache.org/docs/3.5.0/rdd-programming-guide.html#rdd-persistence
        g.vertices.cache()
        g.edges.cache()
        
        # For√ßar materializa√ß√£o
        g.vertices.count()
        g.edges.count()
        
        # M√©tricas do grafo
        avg_degree = (2.0 * edge_count) / vertex_count
        print(f"  üìä M√©tricas:")
        print(f"     ‚Ä¢ Grau m√©dio: {avg_degree:.2f}")
        print(f"     ‚Ä¢ Densidade: {(2.0 * edge_count) / (vertex_count * (vertex_count - 1)):.6f}")
        
        print("  ‚úÖ GraphFrame criado e cacheado")
        return g
        
    except Exception as e:
        print(f"\n‚ùå ERRO ao carregar grafo: {e}")
        raise


def run_pagerank(g: GraphFrame, output_dir: str, max_iter: int = 10) -> dict:
    """
    ‚úÖ MELHORADO: PageRank com m√©tricas detalhadas
    
    Refer√™ncia: https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html#graphframes.GraphFrame.pageRank
    """
    print(f"\nüöÄ Executando PageRank (maxIter={max_iter})...")
    start_time = time.time()
    
    try:
        # PageRank com checkpoint autom√°tico
        results = g.pageRank(resetProbability=0.15, maxIter=max_iter)
        
        # Selecionar e ordenar
        pr_output = results.vertices \
            .select("id", "name", "pagerank", "country", "user_type") \
            .orderBy(F.desc("pagerank"))
        
        # ‚úÖ Salvar com particionamento por pa√≠s
        output_path = f"{output_dir}/pagerank"
        pr_output.write \
            .mode("overwrite") \
            .partitionBy("country") \
            .parquet(output_path)
        
        elapsed = time.time() - start_time
        
        # Estat√≠sticas detalhadas
        stats = pr_output.select(
            F.count("*").alias("total"),
            F.max("pagerank").alias("max_pr"),
            F.mean("pagerank").alias("avg_pr"),
            F.stddev("pagerank").alias("std_pr")
        ).collect()[0]
        
        metrics = {
            'duration': elapsed,
            'total_nodes': stats['total'],
            'max_pagerank': stats['max_pr'],
            'avg_pagerank': stats['avg_pr'],
            'std_pagerank': stats['std_pr']
        }
        
        print(f"  ‚úÖ PageRank conclu√≠do em {elapsed:.2f}s")
        print(f"     ‚Ä¢ N√≥s processados: {metrics['total_nodes']:,}")
        print(f"     ‚Ä¢ PageRank m√°ximo: {metrics['max_pagerank']:.6f}")
        print(f"     ‚Ä¢ PageRank m√©dio: {metrics['avg_pagerank']:.6f}")
        print(f"     ‚Ä¢ Desvio padr√£o: {metrics['std_pagerank']:.6f}")
        print(f"     ‚Ä¢ Salvo em: {output_path}")
        
        return metrics
        
    except Exception as e:
        print(f"  ‚ùå Erro no PageRank: {e}")
        raise


def run_label_propagation(g: GraphFrame, output_dir: str, max_iter: int = 5) -> dict:
    """
    ‚úÖ MELHORADO: LPA com an√°lise de qualidade
    
    Nota: LPA pode ser inst√°vel em grafos scale-free (documentado em an√°lise)
    Refer√™ncia: https://graphframes.github.io/graphframes/docs/_site/user-guide.html#label-propagation-algorithm-lpa
    """
    print(f"\nüèòÔ∏è  Executando Label Propagation (maxIter={max_iter})...")
    start_time = time.time()
    
    try:
        results = g.labelPropagation(maxIter=max_iter)
        
        # Calcular tamanhos de comunidades
        community_sizes = results.groupBy("label") \
            .count() \
            .withColumnRenamed("count", "community_size")
        
        results_enriched = results.join(community_sizes, "label")
        
        # Salvar
        output_path = f"{output_dir}/communities"
        results_enriched.write \
            .mode("overwrite") \
            .partitionBy("label") \
            .parquet(output_path)
        
        elapsed = time.time() - start_time
        
        # Estat√≠sticas
        comm_stats = community_sizes.select(
            F.count("*").alias("num_communities"),
            F.max("community_size").alias("largest"),
            F.mean("community_size").alias("avg_size"),
            F.stddev("community_size").alias("std_size")
        ).collect()[0]
        
        metrics = {
            'duration': elapsed,
            'num_communities': comm_stats['num_communities'],
            'largest_community': comm_stats['largest'],
            'avg_size': comm_stats['avg_size'],
            'std_size': comm_stats['std_size'] if comm_stats['std_size'] else 0
        }
        
        print(f"  ‚úÖ Label Propagation conclu√≠do em {elapsed:.2f}s")
        print(f"     ‚Ä¢ Comunidades: {metrics['num_communities']:,}")
        print(f"     ‚Ä¢ Maior: {metrics['largest_community']:,} membros")
        print(f"     ‚Ä¢ Tamanho m√©dio: {metrics['avg_size']:.1f}")
        print(f"     ‚Ä¢ Salvo em: {output_path}")
        
        # ‚ö†Ô∏è Aviso para grafos scale-free
        if metrics['largest_community'] > metrics['avg_size'] * 10:
            print(f"     ‚ö†Ô∏è  Comunidade dominante detectada (scale-free trait)")
        
        return metrics
        
    except Exception as e:
        print(f"  ‚ùå Erro no Label Propagation: {e}")
        raise


def run_connected_components(g: GraphFrame, output_dir: str) -> dict:
    """
    ‚úÖ MELHORADO: SCC com an√°lise de conectividade
    """
    print(f"\nüîó Executando Connected Components...")
    start_time = time.time()
    
    try:
        results = g.connectedComponents()
        
        component_sizes = results.groupBy("component") \
            .count() \
            .withColumnRenamed("count", "component_size")
        
        results_enriched = results.join(component_sizes, "component")
        
        output_path = f"{output_dir}/connected_components"
        results_enriched.write \
            .mode("overwrite") \
            .parquet(output_path)
        
        elapsed = time.time() - start_time
        
        # Estat√≠sticas
        total_nodes = results.count()
        comp_stats = component_sizes.select(
            F.count("*").alias("num_components"),
            F.max("component_size").alias("largest")
        ).collect()[0]
        
        metrics = {
            'duration': elapsed,
            'num_components': comp_stats['num_components'],
            'largest_component': comp_stats['largest'],
            'largest_pct': (comp_stats['largest'] / total_nodes) * 100
        }
        
        print(f"  ‚úÖ Connected Components conclu√≠do em {elapsed:.2f}s")
        print(f"     ‚Ä¢ Componentes: {metrics['num_components']:,}")
        print(f"     ‚Ä¢ Maior: {metrics['largest_component']:,} n√≥s ({metrics['largest_pct']:.1f}%)")
        print(f"     ‚Ä¢ Salvo em: {output_path}")
        
        return metrics
        
    except Exception as e:
        print(f"  ‚ùå Erro no Connected Components: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="GraphX Community Detection Pipeline",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--input', default='/opt/spark-data/input')
    parser.add_argument('--output', default='/opt/spark-data/output')
    parser.add_argument('--master', default='spark://spark-master:7077')
    parser.add_argument('--shuffle-partitions', type=int, default=None, 
                       help='Se n√£o especificado, usa auto-tuning')
    parser.add_argument('--pagerank-iter', type=int, default=10)
    parser.add_argument('--lpa-iter', type=int, default=5)
    parser.add_argument('--skip-cc', action='store_true')
    parser.add_argument('--no-auto-tune', action='store_true',
                       help='Desabilita auto-tuning de parti√ß√µes')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("üï∏Ô∏è  GraphX Community Detection Engine")
    print("=" * 70)
    print(f"In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    spark = None
    metrics_summary = {}
    
    try:
        # Criar sess√£o
        spark = create_spark_session(
            "GraphX Community Detection",
            args.master,
            args.shuffle_partitions,
            auto_tune=not args.no_auto_tune
        )
        
        # Carregar grafo
        g = load_graph(spark, args.input)
        
        # Executar algoritmos
        total_start = time.time()
        
        metrics_summary['pagerank'] = run_pagerank(g, args.output, args.pagerank_iter)
        metrics_summary['lpa'] = run_label_propagation(g, args.output, args.lpa_iter)
        
        if not args.skip_cc:
            metrics_summary['cc'] = run_connected_components(g, args.output)
        else:
            print("\n‚è≠Ô∏è  Connected Components pulado (--skip-cc)")
        
        total_elapsed = time.time() - total_start
        
        print("\n" + "=" * 70)
        print(f"‚ú® Pipeline finalizado com sucesso!")
        print(f"‚è±Ô∏è  Tempo total: {total_elapsed:.2f}s")
        print(f"üìÅ Resultados: {args.output}")
        
        # Resumo de m√©tricas
        print("\nüìä RESUMO DE M√âTRICAS:")
        for algo, metrics in metrics_summary.items():
            print(f"\n  {algo.upper()}:")
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"    ‚Ä¢ {key}: {value:.2f}")
                else:
                    print(f"    ‚Ä¢ {key}: {value:,}")
        
        print("=" * 70)
        
    except Exception as e:
        print("\n" + "=" * 70)
        print(f"‚ùå Erro fatal no pipeline: {e}")
        print("=" * 70)
        import traceback
        traceback.print_exc()
        sys.exit(1)
        
    finally:
        if spark:
            print("\nüõë Finalizando Spark Session...")
            spark.stop()


if __name__ == "__main__":
    main()